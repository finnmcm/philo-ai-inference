# runpod.yaml
name: llama-4bit-lora
type: python
version: "1.0"
build:
  context: .
  dockerfile: Dockerfile

# entrypoint = your model_fn in inference.py
entrypoint: download_and_run.sh

# expose HTTP on /predict
routes:
  - path: /run
    methods: ["POST"]

# environment variables
env:
  HUGGINGFACE_API_TOKEN: "{{HUGGINGFACE_API_TOKEN}}"  # set in Runpod UI
  HUGGINGFACE_HUB_TOKEN: "{{HUGGINGFACE_API_TOKEN}}"
  AWS_ACCESS_KEY_ID:      ${{AWS_ACCESS_KEY_ID}}
  AWS_SECRET_ACCESS_KEY:  ${{AWS_SECRET_ACCESS_KEY}}
  AWS_DEFAULT_REGION:    "us-east-1"
  MODEL_S3_PATH:          "s3://philo-ai/models/llama8bit/pytorch-training-2025-08-03-03-41-44-353/output/model.tar.gz"

# choose the cheapest Serverless GPU (A10G)
resources:
  gpu:
    type: a40
    concurrency: 1
